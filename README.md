# XAI_SHAP_GradCAM_FashionMNIST
eXplainable AI techniques like SHAP and GradCAM applied on public datasets like FashionMNIST and Fetal Health to gain more insights.


This study explores the interpretability of machine learning models using GradCam and SHAP techniques applied to two distinct datasets. First, the Fashion MNIST dataset is used to explain a basic convolutional neural network (CNN) model. GradCam, a gradient-based visualization method, is employed to highlight the regions of interest within the images that contribute most significantly to the model's predictions. Additionally, SHAP (SHapley Additive exPlanations) is utilized to provide insights into feature importance and decision-making processes of the CNN.

Furthermore, the study extends its analysis to a fetal health classification dataset, employing a random forest model. Here, SHAP is applied to elucidate the random forest's decision mechanism and identify the key features influencing the prediction outcomes related to fetal health. By leveraging these interpretability techniques, the study aims to enhance the transparency and trustworthiness of machine learning models, especially in critical domains like healthcare. The findings underscore the utility of GradCam and SHAP in uncovering model behaviors and improving the understanding of complex datasets and model predictions.


Here are few screenshots for reference:

GradCAM visualization

![image](https://github.com/Dhanush-Mohan/XAI_SHAP_GradCAM_FashionMNIST/assets/115526861/56da4736-0d63-4ed4-8708-6358a78fe383)

SHAP visualization

![image](https://github.com/Dhanush-Mohan/XAI_SHAP_GradCAM_FashionMNIST/assets/115526861/9c37cfd6-1ba4-416f-8bba-2b572275e3b1)

Beeswarm Plot

![image](https://github.com/Dhanush-Mohan/XAI_SHAP_GradCAM_FashionMNIST/assets/115526861/588e1a38-7b62-4900-81cb-f44cbb735575)

Summary Plot

![image](https://github.com/Dhanush-Mohan/XAI_SHAP_GradCAM_FashionMNIST/assets/115526861/f0bd1ffa-e455-4573-a82c-1cec8e4a5a05)
